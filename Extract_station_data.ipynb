{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For convenience: Scrape StationIDs to lookup cities\n",
    "Simple Python script for scraping StationIDs from Environment Canada using Beautiful Soup.\n",
    "\n",
    "The stationIDs are provided by province in this Environment Canada [page](http://climate.weather.gc.ca/historical_data/search_historic_data_e.html). Environment Canada limits the number of rows in the search results to 100 entries. This script loops through all pages and grabs the StationID, Station Name, Intervals and Year Range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need `fuzzywuzzy` to look up weather stations later\n",
    "# Run \"!pip install fuzzywuzzy --user\" if you get an error\n",
    "\n",
    "# !pip install fuzzywuzzy --user\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Environment Canada page with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_station_id(soup_frames, province, data_folder):\n",
    "    # Empty list to store the station data\n",
    "    station_data = []\n",
    "\n",
    "    for soup in soup_frames:  # For each soup\n",
    "        forms = soup.findAll(\n",
    "            \"form\", {\"id\": re.compile('stnRequest*')}\n",
    "        )  # We find the forms with the stnRequest* ID using regex\n",
    "        for form in forms:\n",
    "            try:\n",
    "                # The stationID is a child of the form\n",
    "                station = form.find(\"input\", {\"name\": \"StationID\"})['value']\n",
    "\n",
    "                # The station name is a sibling of the input element named lstProvince\n",
    "                name = (\n",
    "                    form.find(\"input\", {\"name\": \"lstProvince\"})\n",
    "                    .find_next_siblings(\"div\")[0]\n",
    "                    .text\n",
    "                )\n",
    "\n",
    "                # The intervals are listed as children in a 'select' tag named timeframe\n",
    "                timeframes = form.find(\"select\", {\"name\": \"timeframe\"}).findChildren()\n",
    "                intervals = [t.text for t in timeframes]\n",
    "\n",
    "                # We can find the min and max year of this station using the first and last child\n",
    "                years = form.find(\"select\", {\"name\": \"Year\"}).findChildren()\n",
    "                min_year = years[0].text\n",
    "                max_year = years[-1].text\n",
    "\n",
    "                # Store the data in an array\n",
    "                data = [station, name, intervals, min_year, max_year, province]\n",
    "                station_data.append(data)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Create a pandas dataframe using the collected data and give it the appropriate column names\n",
    "    stations_df = pd.DataFrame(\n",
    "        station_data,\n",
    "        columns=[\n",
    "            'StationID',\n",
    "            'Name',\n",
    "            'Intervals',\n",
    "            'Year Start',\n",
    "            'Year End',\n",
    "            'Province',\n",
    "        ],\n",
    "    )\n",
    "    # stations_df.head()\n",
    "    stations_df.to_csv(f'{data_folder}stations_{province}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Page: 0 for province AB\n",
      "Downloading Page: 1 for province AB\n",
      "Downloading Page: 2 for province AB\n",
      "Downloading Page: 3 for province AB\n",
      "Downloading Page: 0 for province BC\n",
      "Downloading Page: 1 for province BC\n",
      "Downloading Page: 2 for province BC\n",
      "Downloading Page: 3 for province BC\n",
      "Downloading Page: 0 for province MB\n",
      "Downloading Page: 1 for province MB\n",
      "Downloading Page: 0 for province NB\n",
      "Downloading Page: 0 for province NL\n",
      "Downloading Page: 0 for province NT\n",
      "Downloading Page: 0 for province NS\n",
      "Downloading Page: 0 for province NU\n",
      "Downloading Page: 1 for province NU\n",
      "Downloading Page: 0 for province ON\n",
      "Downloading Page: 1 for province ON\n",
      "Downloading Page: 2 for province ON\n",
      "Downloading Page: 3 for province ON\n",
      "Downloading Page: 0 for province PE\n",
      "Downloading Page: 0 for province QC\n",
      "Downloading Page: 1 for province QC\n",
      "Downloading Page: 2 for province QC\n",
      "Downloading Page: 3 for province QC\n",
      "Downloading Page: 0 for province SK\n",
      "Downloading Page: 1 for province SK\n",
      "Downloading Page: 0 for province YT\n"
     ]
    }
   ],
   "source": [
    "# Specify Parameters\n",
    "provinces = [\n",
    "    \"AB\",\n",
    "    \"BC\",\n",
    "    \"MB\",\n",
    "    \"NB\",\n",
    "    \"NL\",\n",
    "    \"NT\",\n",
    "    \"NS\",\n",
    "    \"NU\",\n",
    "    \"ON\",\n",
    "    \"PE\",\n",
    "    \"QC\",\n",
    "    \"SK\",\n",
    "    \"YT\",\n",
    "]  # Province list\n",
    "max_pages = [\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    4,\n",
    "    1,\n",
    "    4,\n",
    "    2,\n",
    "    1,\n",
    "]  #  Number of pages knowing that each display 100 rows\n",
    "nb_stations = [\n",
    "    319,\n",
    "    367,\n",
    "    107,\n",
    "    40,\n",
    "    95,\n",
    "    99,\n",
    "    67,\n",
    "    124,\n",
    "    308,\n",
    "    17,\n",
    "    332,\n",
    "    117,\n",
    "    48,\n",
    "]  # number of stations since 2014\n",
    "start_year = \"2014\"  # I want the results to go back to at least 2006 or earlier\n",
    "data_folder = 'data/stations/'\n",
    "\n",
    "\n",
    "for idx, province in enumerate(provinces):\n",
    "    # Store each page in a list and parse them later\n",
    "    soup_frames = []\n",
    "    for i in range(max_pages[idx]):\n",
    "        startRow = 1 + i * 100\n",
    "        print(f'Downloading Page: {i} for province {province}')\n",
    "\n",
    "        base_url = \"http://climate.weather.gc.ca/historical_data/search_historic_data_stations_e.html?\"\n",
    "        queryProvince = (\n",
    "            f\"searchType=stnProv&timeframe=1&lstProvince={province}&optLimit=yearRange&\"\n",
    "        )\n",
    "        queryYear = f\"StartYear={start_year}&EndYear=2024&Year=2024&Month=7&Day=20&selRowPerPage=100&txtCentralLatMin=0&txtCentralLatSec=0&txtCentralLongMin=0&txtCentralLongSec=0&\"\n",
    "        queryStartRow = f\"startRow={startRow}\"\n",
    "\n",
    "        response = requests.get(\n",
    "            base_url + queryProvince + queryYear + queryStartRow\n",
    "        )  # Using requests to read the HTML source\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  # Parse with Beautiful Soup\n",
    "        soup_frames.append(soup)\n",
    "    parse_station_id(soup_frames, province, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Qu√©bec and Ontario data cause we'll use them for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "dfs: List[pd.DataFrame] = []\n",
    "dfs.append(pd.read_csv(f'{data_folder}stations_QC.csv'))\n",
    "dfs.append(pd.read_csv(f'{data_folder}stations_ON.csv'))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv(f'{data_folder}stations_QC_ON.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the stations currently active (in 2024) and keep only the stations with later 'Start Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Intervals</th>\n",
       "      <th>Year Start</th>\n",
       "      <th>Year End</th>\n",
       "      <th>Province</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54067</td>\n",
       "      <td>AKULIVIK A</td>\n",
       "      <td>['Hourly']</td>\n",
       "      <td>2015</td>\n",
       "      <td>2024</td>\n",
       "      <td>QC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54068</td>\n",
       "      <td>AKULIVIK A</td>\n",
       "      <td>['Hourly', 'Daily']</td>\n",
       "      <td>2018</td>\n",
       "      <td>2024</td>\n",
       "      <td>QC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5310</td>\n",
       "      <td>ARTHABASKA</td>\n",
       "      <td>['Daily', 'Monthly']</td>\n",
       "      <td>1969</td>\n",
       "      <td>2024</td>\n",
       "      <td>QC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5575</td>\n",
       "      <td>ARUNDEL</td>\n",
       "      <td>['Daily', 'Monthly']</td>\n",
       "      <td>1963</td>\n",
       "      <td>2024</td>\n",
       "      <td>QC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54038</td>\n",
       "      <td>AUPALUK A</td>\n",
       "      <td>['Hourly']</td>\n",
       "      <td>2015</td>\n",
       "      <td>2024</td>\n",
       "      <td>QC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StationID        Name             Intervals  Year Start  Year End Province\n",
       "0      54067  AKULIVIK A            ['Hourly']        2015      2024       QC\n",
       "1      54068  AKULIVIK A   ['Hourly', 'Daily']        2018      2024       QC\n",
       "5       5310  ARTHABASKA  ['Daily', 'Monthly']        1969      2024       QC\n",
       "6       5575     ARUNDEL  ['Daily', 'Monthly']        1963      2024       QC\n",
       "8      54038   AUPALUK A            ['Hourly']        2015      2024       QC"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df = pd.read_csv(f'{data_folder}stations_QC_ON.csv')\n",
    "stations_df_2024 = stations_df.loc[stations_df['Year End'] == 2024]\n",
    "\n",
    "stations_df_2024.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_2024.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_stations_name = stations_df_2024[\n",
    "    stations_df_2024.duplicated('Name', keep=False) == True\n",
    "]\n",
    "single_stations_name = stations_df_2024[\n",
    "    stations_df_2024.duplicated('Name', keep=False) == False\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = duplicate_stations_name.groupby('Name')['Year Start'].idxmax()\n",
    "max_year = duplicate_stations_name.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs: List[pd.DataFrame] = []\n",
    "dfs.append(single_stations_name)\n",
    "dfs.append(max_year)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv(f'{data_folder}clean_stations_QC_ON_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SATATION_ID = df['StationID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select just the Ontario data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Province'] == 'ON']\n",
    "df.to_csv(f'{data_folder}clean_stations_ON_2024.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_course",
   "language": "python",
   "name": "mlops_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
